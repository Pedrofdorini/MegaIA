{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef19662-411a-46b3-a91e-6151f14e8b51",
   "metadata": {},
   "source": [
    "# Fazendo a extração de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f62854-0f86-4f25-bc46-ae69b4457333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_adds(concurso):\n",
    "    url = f\"https://www.megasena.com/resultados/{concurso}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs(response.content, 'html.parser')\n",
    "\n",
    "        #pegar a arrecadação total\n",
    "        winners_box = soup.find('div', class_='gen-box winners')\n",
    "        if winners_box:\n",
    "            arrecadacao= winners_box.find('div', class_='sub-title').get_text(strip=True)\n",
    "        else:\n",
    "            arrecadacao=None\n",
    "\n",
    "        #informação final\n",
    "        inners_box_2= soup.find('div', class_=\"inner-box elem-2\")\n",
    "        if inners_box_2:\n",
    "            gen_boxes= inners_box_2.find_all('div',class_=\"gen-box\")\n",
    "            row_data={}\n",
    "            \n",
    "            for gen_box in gen_boxes:\n",
    "                title=gen_box.find('div',class_=\"title\").get_text(strip=True)\n",
    "                sub_title= gen_box.find('div',class_=\"sub-title\").get_text(strip=True)\n",
    "\n",
    "                row_data[title]=sub_title\n",
    "                row_data[\"arrecadação\"]=arrecadacao\n",
    "                row_data[\"Número do Concurso\"]= concurso\n",
    "\n",
    "            return [row_data]\n",
    "        else:\n",
    "            return {\"Número do Concurso\": concurso, \"arrecadação\": arrecadacao}\n",
    "    else:\n",
    "        print(f\"Não foi possível estabelecer uma requisição HTTP, código de erro: {response.status_code}\")\n",
    "        return None\n",
    "        \n",
    "def fetch_table(concurso):\n",
    "    url = f\"https://www.megasena.com/resultados/{concurso}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs(response.content, 'html.parser')                  \n",
    "        # Pegar a data do concurso\n",
    "        h1_tag = soup.find(\"h1\")\n",
    "        if h1_tag:\n",
    "            span_tag = h1_tag.find(\"span\")\n",
    "            data_concurso = span_tag.text.strip()\n",
    "        else:\n",
    "            data_concurso = None\n",
    "        \n",
    "        # Extrair os números sorteados\n",
    "        balls = soup.find_all(\"li\", class_=\"ball\")\n",
    "        if len(balls) == 6:\n",
    "            numeros_sorteados = [ball.text.strip() for ball in balls]\n",
    "        else:\n",
    "            numeros_sorteados = None\n",
    "\n",
    "        table = soup.find(\"table\", class_=\"_numbers -right table-col-4 mobFormat\")\n",
    "        if table:\n",
    "            headers = [th.text.strip() for th in table.find_all('th')]\n",
    "            rows = table.find_all('tr')[1:]  # Skip header row\n",
    "\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if cells:\n",
    "                    row_data = {headers[i]: cells[i].text.strip() for i in range(len(cells))}\n",
    "                    row_data[\"Data do Concurso\"] = data_concurso\n",
    "                    row_data[\"Número do Concurso\"]= concurso\n",
    "                    row_data[\"Números Sorteados\"] = \", \".join(numeros_sorteados) if numeros_sorteados else None\n",
    "                    data.append(row_data)\n",
    "\n",
    "            return data\n",
    "        else:\n",
    "            print(\"Tabela não encontrada na página.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Não foi possível estabelecer uma requisição HTTP, código de erro: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def fetch_ganhadores_por_estado(concurso):\n",
    "    url = f\"https://www.megasena.com/resultados/{concurso}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = bs(response.content, 'html.parser')\n",
    "\n",
    "        table = soup.find(\"table\", class_=\"_numbers -right mobFormat\")\n",
    "        if table:\n",
    "            headers = [th.text.strip() for th in table.find_all('th')]\n",
    "            rows = table.find_all('tr')[1:]  # Skip header row\n",
    "\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                cells = row.find_all('td')\n",
    "                if cells:\n",
    "                    row_data = {headers[i]: cells[i].text.strip() for i in range(len(cells))}\n",
    "                    data.append(row_data)\n",
    "\n",
    "            return data\n",
    "        else:\n",
    "            print(\"Tabela de ganhadores por estado não encontrada na página.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Não foi possível estabelecer uma requisição HTTP, código de erro: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def save_to_excel(data_principal, data_ganhadores,data_adicional, filename):\n",
    "    df_principal = pd.DataFrame(data_principal)\n",
    "    df_ganhadores = pd.DataFrame(data_ganhadores)\n",
    "    df_adicional= pd.DataFrame(data_adicional)\n",
    "    \n",
    "    with pd.ExcelWriter(filename) as writer:\n",
    "        df_principal.to_excel(writer, sheet_name=\"dados gerais\", index=False)\n",
    "        df_ganhadores.to_excel(writer, sheet_name=\"Ganhadores por Estado\", index=False)\n",
    "        df_adicional.to_excel(writer,sheet_name= \"Dados isolados\", index=False)\n",
    "        \n",
    "    \n",
    "    print(f\"Dados salvos no arquivo {filename}\")\n",
    "\n",
    "def main():\n",
    "    all_data_principal = []\n",
    "    all_data_ganhadores = []\n",
    "    all_data_adicional = []\n",
    "    concursos=list(range(2760,0,-1))\n",
    "    for concurso in tqdm(concursos, desc=\"Collecting Data\"):  # Intervalo de concursos em ordem decrescente\n",
    "        data_principal = fetch_table(concurso)\n",
    "        if data_principal:\n",
    "            all_data_principal.extend(data_principal)  # Acumular dados de todos os concursos\n",
    "        \n",
    "        data_ganhadores = fetch_ganhadores_por_estado(concurso)\n",
    "        if data_ganhadores:\n",
    "            for item in data_ganhadores:\n",
    "                item[\"Concurso\"] = concurso\n",
    "            all_data_ganhadores.extend(data_ganhadores)  # Acumular dados de ganhadores por estado\n",
    "        data_adicional = fetch_adds(concurso)\n",
    "        if data_adicional:\n",
    "            all_data_adicional.extend(data_adicional)\n",
    "\n",
    "        \n",
    "\n",
    "    if all_data_principal and all_data_ganhadores and all_data_adicional:\n",
    "        save_to_excel(all_data_principal, all_data_ganhadores,all_data_adicional ,\"megasena_resultados.xlsx\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49101f0c-a8ca-45cc-8d24-b0ae2477987e",
   "metadata": {},
   "source": [
    "# IA training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d7fa4fb-2da7-40b6-9300-3950b35955f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "     --------------------------------------- 11.0/11.0 MB 43.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\user\\onedrive\\área de trabalho\\data\\megacena\\megaia\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.14.0-cp311-cp311-win_amd64.whl (44.7 MB)\n",
      "     --------------------------------------- 44.7/44.7 MB 21.8 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "     -------------------------------------- 301.8/301.8 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.14.0 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a0cb6-36a7-4df4-8d84-47ceacbd896c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (megaia)",
   "language": "python",
   "name": "megaia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
